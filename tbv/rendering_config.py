"""
Copyright (c) 2021
Argo AI, LLC, All Rights Reserved.

Notice: All information contained herein is, and remains the property
of Argo AI. The intellectual and technical concepts contained herein
are proprietary to Argo AI, LLC and may be covered by U.S. and Foreign
Patents, patents in process, and are protected by trade secret or
copyright law. This work is licensed under a CC BY-NC-SA 4.0 
International License.

Originating Authors: John Lambert
"""

from dataclasses import dataclass
from enum import Enum
from typing import Optional, Union

import hydra
from hydra.utils import instantiate

# todo: put default args at the end

class SensorViewpoint(str, Enum):
    """ """
    EGOVIEW = "EGOVIEW"
    BEV = "BEV"


@dataclass
class RenderingConfig:
    """Holds options for rendering training data in the ego-view or BEV.

    Args:
        num_processes: number of workers to use for concurrent rendering (one worker per log).
        render_reflectance: whether to generate LiDAR intensity BEV images.
        recompute_segmentation:
        range_m: maximum range from egovehicle to consider for rendering (by l-infinity norm).
        tbv_dataroot: path to root TbV Dataset directory.
        rendered_dataset_dir: 
        seamseg_output_dataroot: path to where inferred `seamseg` label maps are located on disk.

        render_test_set_only: whether to render without any synthetic data augmentation (for eval only).
        jitter_vector_map:
        max_dist_to_del_crosswalk:
    """

    # could differentiate between CPU and GPU processes, TODO: MUST BE MULTIPLE OF 8
    num_processes: int
    render_reflectance: bool
    recompute_segmentation: bool

    range_m: float  # 20 m in all directions

    tbv_dataroot: str
    rendered_dataset_dir: str
    seamseg_output_dataroot: str

    render_test_set_only: bool
    jitter_vector_map: bool
    max_dist_to_del_crosswalk: float


@dataclass
class EgoviewRenderingConfig(RenderingConfig):
    """Specification for rendering a dataset in the ego-view.
    
    Args:
        use_depth_map_for_occlusion: whether to mask map entities when rendering. The occlusion mask is
            generated by interpolation on a sparse LiDAR depth map, and comparing depth z-values w/ map
            entity z-values.
    """

    use_depth_map_for_occlusion: bool

    @property
    def viewpoint(self) -> str:
        """Return the dataset rendering perspective."""
        return SensorViewpoint.EGOVIEW


@dataclass
class BevRenderingConfig(RenderingConfig):
    """Specification for rendering a dataset in the bird's eye view (BEV).

    Args:
        projection_method: for BEV only, either to use 'ray_tracing' for correspondences, or 'lidar_projection'
            TODO: make it a enum
        render_vector_map_only: whether to render only the map (i.e. not render the BEV sensor data).
        use_histogram_matching
        make_bev_semantic_img: whether to generate BEV semantic image, raycasting and painting w/ semantic seg.
        render_reflectance_only:
        filter_ground_with_semantics: filter away non-ground 3d points by comparing w/ ground-specific classes
            from a semantic label map.
        filter_ground_with_map: filter away 3d points by comparing z-values with ground surface height map values.
        max_ground_mesh_range_m:
        resolution_m_per_px: resolution to use when rendering, in meters/pixel.
        use_median_filter:
        sensor_img_interp_type: string representing type of interpolation to apply to senor data representation
            (can be "None")
        semantic_img_interp_type:
        sensor_modality:
    """

    projection_method: str
    render_vector_map_only: bool
    use_histogram_matching: bool
    make_bev_semantic_img: bool  # whether to render RGB or semantics in BEV
    render_reflectance_only: bool
    filter_ground_with_semantics: bool  # quite expensive
    filter_ground_with_map: bool  # using ground height
    max_ground_mesh_range_m: float  # trace rays to triangles up to 20 m away
    resolution_m_per_px: float  # meters/pixel
    use_median_filter: bool
    sensor_img_interp_type: str  # linear for rgb, but nearest for label map, or "None"
    # linear for rgb, but nearest for label map, if not rendering semantics, optional
    semantic_img_interp_type: Optional[str] = None
    sensor_modality: str = "rgb"  # `rgb` or `reflectance`?

    @property
    def viewpoint(self) -> str:
        """Return the dataset rendering perspective."""
        return SensorViewpoint.BEV


def load_bev_rendering_config(config_name: str) -> BevRenderingConfig:
    """Get experiment config for rendering maps and sensor data in a bird's eye view."""
    with hydra.initialize_config_module(config_module="tbv.rendering_configs"):
        # config is relative to the tbv module
        cfg = hydra.compose(config_name=config_name)
        config: BevRenderingConfig = instantiate(cfg.BevRenderingConfig)

    return config


def load_egoview_rendering_config(config_name: str) -> EgoviewRenderingConfig:
    """Get experiment config for rendering maps and sensor data in the ego-view."""
    with hydra.initialize_config_module(config_module="tbv.rendering_configs"):
        # config is relative to the tbv module
        cfg = hydra.compose(config_name=config_name)
        config: EgoviewRenderingConfig = instantiate(cfg.EgoviewRenderingConfig)

    return config


def load_rendering_config(config_name: str) -> Union[EgoviewRenderingConfig, BevRenderingConfig]:
    """Load from disk the parameters for rendering sensor+map data (from a yaml file)."""
    try:
        config = load_egoview_rendering_config(config_name)
    except:
        config = load_bev_rendering_config(config_name)

    # if not (isinstance(config, EgoviewRenderingConfig) or isinstance(config, BevRenderingConfig)):
    #     raise RuntimeError("Invalid config.")

    if not config:
        raise RuntimeError("Invalid config.")

    return config
